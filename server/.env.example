## Example environment for the server (do NOT commit real secrets here)
# Copy this to server/.env and fill values locally.

# LLM provider settings
# If you want to use Ollama (local), set OLLAMA_URL to your Ollama HTTP API (example below)
OLLAMA_URL=http://127.0.0.1:11434
# Optional: specify an Ollama model (leave blank to use Ollama default)
OLLAMA_MODEL=

# If you prefer to use OpenAI instead of Ollama, set your OpenAI key here.
# IMPORTANT: do NOT commit your real key. Keep it only in server/.env which is ignored.
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Quick local testing: if true, the server returns canned replies and doesn't call a provider
MOCK_GEMINI=false

# Security: a short token the front-end uses to talk to this proxy (dev default shown)
APP_CLIENT_TOKEN=dev-token-change-me

# Server bind
PORT=3123
HOST=127.0.0.1

# End of example
