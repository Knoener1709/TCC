## Example environment for the server (do NOT commit real secrets here)
# Copy this to server/.env and fill values locally.

# LLM provider settings
# If you want to use a local text-generation-webui (oobabooga), set its API URL here
# (the web UI can be started with `--api` and typically listens on port 5000)
WEBUI_URL=http://127.0.0.1:5000
# Optional: specify a model name for the web UI (leave blank to use the webui default)
WEBUI_MODEL=

# If you prefer to use OpenAI instead of a local web UI, set your OpenAI key here.
# IMPORTANT: do NOT commit your real key. Keep it only in server/.env which is ignored.
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Enable a tiny rule-based bot (no model) for very specific commands. Useful for fast deterministic replies.
# When true, the server will answer common commands like "hora", "data", "piada", "ajuda" locally.
SIMPLE_BOT=false

# Quick local testing: if true, the server returns canned replies and doesn't call a provider
MOCK_GEMINI=false

# Security: a short token the front-end uses to talk to this proxy (dev default shown)
APP_CLIENT_TOKEN=dev-token-change-me

# Server bind
PORT=3123
HOST=127.0.0.1

# End of example
